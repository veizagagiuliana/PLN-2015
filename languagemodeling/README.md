PRACTICO 1

Ejercicio 3 : Generación de texto

n-grama:
n = 2:
* Al día le llevaba catleyas . * “ Renata de que representaban los días le pedí noticias de lo encontraban en mi abuela esas seducciones que conocía sus brazos en una frase que había observado entonces deben enamorarse más esa regla de Charlus , para limpiarlo , sino que al no verla . * Y de René : Agudo y que al arrugar un sello de su camino junto al bridge . * En verdad , ha de hipo , por lo que me habían nacido Legrandin llegó también porque ese efecto , y una cola de fastidio que está todavía la Gravière : un hombre ordinario .

n = 3:
* No bien se dijera que podía expresar para un francés excelente .
* Me extrañó extraordinariamente ver cómo se las había olvidado mi pepsina .
* Y me parece una Librodot En busca del tiempo perdido II .
* Porque en ella ».

n = 4:
* En busca del tiempo perdido III .
* Dégrange , que acaba de juzgar retrospectivamente un incidente liquidado .
* De lejos veía el jardincillo de los Swann antes de introducirme en el salón de baile del pequeño casino y como si se tratase de algo importante y raro que impusiera deferencia y reclamara atención .
* Ese doble rasero para medir el ingenio , hay que salir al empezar la cena , a un lento y difícil aclararse , el placer que iba yo a reprochar a la Revolución el no haberlos guillotinado a todos .


Ejercicio 5 : Evaluación de Modelos de Lenguaje

n-gram: 
n = 1:
log_probability = -877902.1995094967,
cross_entropy = 9.854436668756348,
perplexity = 925.7229436494428

n=2,3,4:
log_probability = -inf,
cross_entropy = inf,
perplexity = inf
# no encuentro el problema y hoy a la mañana funcionaba


Ejercicio 6 : Suavizado por Interpolación

n-gram: 
n = 1:
log_probability = -883040.9202161023,
cross_entropy = 9.912118717838768,
perplexity = 963.4852150045497

n = 2:
log_probability = -774388.9985659586,
cross_entropy = 8.692502818210947,
perplexity = 413.71768480564936

